# app.py
# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from scipy.stats import spearmanr
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestRegressor

import prince  # MCA, CA, FAMD


# ========================
# CONFIGURACI√ìN APP
# ========================
st.set_page_config(page_title="üè• Hospital Admissions", layout="wide")
st.title("üè• Hospital Admissions Analysis")
st.markdown("Aplicaci√≥n convertida desde Google Colab ‚Üí Streamlit ‚úîÔ∏è")


# ========================
# CARGA DE DATOS
# ========================
@st.cache_data
def load_data():
    """
    Carga los datos directamente desde la URL de GitHub.
    Si hay un error en la carga, retorna None.
    """
    url = "https://raw.githubusercontent.com/Juansebastianrde/Reduccion-de-dimensionalidad/main/HDHI%20Admission%20data.csv"
    try:
        df = pd.read_csv(url, sep=",", engine="python")  # üëà Aseguramos que es coma
        return df
    except Exception as e:
        st.error(f"Error al cargar la base de datos: {e}")
        return None


df = load_data()

if df is not None:
    st.success("‚úÖ Datos cargados correctamente desde GitHub")

    st.subheader("üëÄ Vista previa")
    st.write("Dimensiones del dataset:", df.shape)
    st.dataframe(df.head(), use_container_width=True)

    st.subheader("üìã Tipos de variables")
    st.write(df.dtypes)

else:
    st.warning("‚ö†Ô∏è No se pudieron cargar los datos desde GitHub")

# ========================
# PREPROCESAMIENTO DE LA BASE
# ========================
# ========================
# PREPROCESAMIENTO DE LA BASE
# ========================
@st.cache_data
def preprocess_data(bd: pd.DataFrame) -> pd.DataFrame:
    """Limpieza y transformaci√≥n del dataset hospitalario"""

    # Eliminar variable BNP (demasiados nulos / ruido)
    if "BNP" in bd.columns:
        bd = bd.drop("BNP", axis=1)

    # Eliminar variables innecesarias
    cols_to_drop = [c for c in ["SNO", "MRD No.", "month year"] if c in bd.columns]
    df = bd.drop(columns=cols_to_drop)

    # Fechas a datetime
    if "D.O.A" in df.columns:
        df["D.O.A"] = pd.to_datetime(df["D.O.A"], format="%m/%d/%Y", errors="coerce")
    if "D.O.D" in df.columns:
        df["D.O.D"] = pd.to_datetime(df["D.O.D"], format="%m/%d/%Y", errors="coerce")

    # Variables num√©ricas que vienen como texto
    cols_to_clean = ["HB", "TLC", "PLATELETS", "GLUCOSE", "UREA", "CREATININE", "EF"]
    for col in cols_to_clean:
        if col in df.columns:
            df[col] = (
                df[col]
                .astype(str)
                .str.strip()
                .replace(["EMPTY", "nan", "NaN", "None", ""], np.nan)
                .str.replace(r"[<>]", "", regex=True)
                .str.replace(",", ".", regex=False)
            )
            df[col] = pd.to_numeric(df[col], errors="coerce")

    # Variables categ√≥ricas ‚Üí binarias
    if "GENDER" in df.columns:
        df["GENDER"] = df["GENDER"].map({"M": 1, "F": 0})
    if "RURAL" in df.columns:
        df["RURAL"] = df["RURAL"].map({"R": 1, "U": 0})
    if "TYPE OF ADMISSION-EMERGENCY/OPD" in df.columns:
        df["TYPE OF ADMISSION-EMERGENCY/OPD"] = df["TYPE OF ADMISSION-EMERGENCY/OPD"].map(
            {"E": 1, "O": 0}
        )
    if "CHEST INFECTION" in df.columns:
        df["CHEST INFECTION"] = df["CHEST INFECTION"].map({"1": 1, "0": 0})

    # Dummies para outcome
    if "OUTCOME" in df.columns:
        df = pd.get_dummies(df, columns=["OUTCOME"], drop_first=False)

    # Booleanas a int
    bool_cols = df.select_dtypes(include=bool).columns
    df[bool_cols] = df[bool_cols].astype(int)

    # Eliminar variable "duration of intensive unit stay"
    if "duration of intensive unit stay" in df.columns:
        df = df.drop("duration of intensive unit stay", axis=1)

    return df


# ========================
# USO EN LA APP
# ========================
st.header("üìä Preprocesamiento de los datos")

df_raw = load_data()
if df_raw is not None:
    st.success("‚úÖ Datos cargados correctamente desde GitHub")
    st.write("Dimensiones iniciales:", df_raw.shape)

    st.markdown("""
    ### üîπ Paso 1: Eliminaci√≥n de variables irrelevantes  
    Se eliminan:  
    - `BNP` (muchos nulos / ruido).  
    - Identificadores internos: `SNO`, `MRD No.`.  
    - Columna `month year`.  
    """)

    st.markdown("""
    ### üîπ Paso 2: Conversi√≥n de fechas  
    Las variables `D.O.A` (fecha de admisi√≥n) y `D.O.D` (fecha de alta) se transforman al formato **datetime**.
    """)

    st.markdown("""
    ### üîπ Paso 3: Limpieza de variables num√©ricas  
    Columnas como hemoglobina (HB), glucosa, creatinina, etc. conten√≠an valores como `"EMPTY"`, `"<12"`, o con comas decimales.  
    Estas se normalizan y convierten a num√©ricas reales.
    """)

    st.markdown("""
    ### üîπ Paso 4: Transformaci√≥n de variables categ√≥ricas  
    - `GENDER`: M=1, F=0  
    - `RURAL`: R=1, U=0  
    - `TYPE OF ADMISSION-EMERGENCY/OPD`: E=1, O=0  
    - `CHEST INFECTION`: 1/0  
    - `OUTCOME`: convertido a variables dummies  
    """)

    st.markdown("""
    ### üîπ Paso 5: Eliminaci√≥n de informaci√≥n no disponible en el ingreso  
    La variable **`duration of intensive unit stay`** se elimina porque no se conoce al momento del ingreso del paciente.
    """)

    # Preprocesar
    df = preprocess_data(df_raw)
    st.success("‚úÖ Datos preprocesados correctamente")
    st.write("Dimensiones despu√©s del preprocesamiento:", df.shape)

    # Vista previa
    st.subheader("üëÄ Vista previa del dataset procesado")
    st.dataframe(df.head(), use_container_width=True)

else:
    st.warning("‚ö†Ô∏è No se pudieron cargar los datos desde GitHub")

# ========================
# EXPLORACI√ìN INICIAL (EDA)
# ========================
st.header("üîç Exploraci√≥n de Datos (EDA)")

# Eliminar columna que no sirve
if "duration of intensive unit stay" in df.columns:
    df = df.drop("duration of intensive unit stay", axis=1)

# Quitar espacios en los nombres de columnas
df.columns = df.columns.str.strip()

# ========================
# Separaci√≥n en categ√≥ricas y num√©ricas
# ========================
st.subheader("üìë Separaci√≥n de variables")

cat_features = [
    'GENDER', 'RURAL', 'TYPE OF ADMISSION-EMERGENCY/OPD',
    'OUTCOME_DAMA', 'OUTCOME_DISCHARGE', 'OUTCOME_EXPIRY',
    'SMOKING', 'ALCOHOL', 'DM', 'HTN', 'CAD', 'PRIOR CMP', 'CKD',
    'RAISED CARDIAC ENZYMES', 'SEVERE ANAEMIA', 'ANAEMIA', 'STABLE ANGINA',
    'ACS', 'STEMI', 'ATYPICAL CHEST PAIN', 'HEART FAILURE', 'HFREF', 'HFNEF',
    'VALVULAR', 'CHB', 'SSS', 'AKI', 'CVA INFRACT', 'CVA BLEED', 'AF', 'VT', 'PSVT',
    'CONGENITAL', 'UTI', 'NEURO CARDIOGENIC SYNCOPE', 'ORTHOSTATIC',
    'INFECTIVE ENDOCARDITIS', 'DVT', 'CARDIOGENIC SHOCK', 'SHOCK',
    'PULMONARY EMBOLISM', 'CHEST INFECTION'
]

num_features = [col for col in df.columns if col not in cat_features and col not in ['D.O.A', 'D.O.D', 'DURATION OF STAY']]

df_numericas = df[num_features]

st.markdown(f"""
- üî¢ Variables **num√©ricas detectadas**: `{len(num_features)}`  
- üßæ Variables **categ√≥ricas detectadas**: `{len(cat_features)}`
""")

# Mostrar ejemplos
st.write("üëÄ Vista previa de variables num√©ricas:")
st.dataframe(df_numericas.head(), use_container_width=True)

# ========================
# Boxplots de las variables num√©ricas
# ========================
st.subheader("üìä Distribuci√≥n y posibles outliers (Boxplots)")

st.info("A continuaci√≥n se muestran diagramas de caja para cada variable num√©rica con el fin de identificar valores at√≠picos.")

import seaborn as sns
import matplotlib.pyplot as plt

fig, axes = plt.subplots(4, 4, figsize=(20, 15))
axes = axes.flatten()

for i, col in enumerate(df_numericas):
    sns.boxplot(x=df[col], ax=axes[i], color="skyblue")
    axes[i].set_title(col, fontsize=10)
    axes[i].tick_params(axis="x", rotation=45)

# Eliminar ejes vac√≠os si sobran
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
st.pyplot(fig)

# ========================
# OUTLIERS Y ASIMETR√çA
# ========================
st.header("üìå An√°lisis de Outliers y Distribuciones")

# ------------------------
# 1. Calcular outliers con IQR
# ------------------------
outliers_list = []
for c in num_features:
    Q1 = df[c].quantile(0.25)
    Q3 = df[c].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR

    mask = (df[c] < lower) | (df[c] > upper)

    temp = (
        df.loc[mask, [c]]
        .rename(columns={c: "value"})
        .assign(
            variable=c,
            lower_bound=lower,
            upper_bound=upper,
            row_index=lambda x: x.index,
        )
    )

    outliers_list.append(temp)

outliers = pd.concat(outliers_list, ignore_index=True)
resumen = outliers.groupby("variable").size().reset_index(name="n_outliers")
resumen["pct_outliers"] = (resumen["n_outliers"] / len(df)) * 100

st.subheader("üìä Porcentaje de outliers por variable")
st.dataframe(resumen, use_container_width=True)

# ------------------------
# 2. Skewness (asimetr√≠a)
# ------------------------
from scipy.stats import skew

asimetria_pandas = df_numericas.skew().sort_values(ascending=False)
altamente_asimetricas = asimetria_pandas[abs(asimetria_pandas) > 2]

st.subheader("üìà Asimetr√≠a (Skewness) de variables num√©ricas")
st.write("Valores positivos indican cola larga a la derecha; negativos, cola a la izquierda.")
st.dataframe(asimetria_pandas, use_container_width=True)

st.warning("Variables con |asimetr√≠a| > 2 (fuertemente sesgadas):")
st.write(altamente_asimetricas)

# ------------------------
# 3. Histogramas
# ------------------------
st.subheader("üìâ Histogramas de variables num√©ricas")
fig, ax = plt.subplots(figsize=(12, 8))
df[num_features].hist(bins=50, figsize=(12, 8))
st.pyplot(fig)

# ------------------------
# 4. Interpretaci√≥n textual
# ------------------------
st.markdown("""
### üìù Interpretaci√≥n de distribuciones

**AGE (edad)**  
- Distribuci√≥n aproximadamente normal con ligera asimetr√≠a a la derecha.  
- Mayor concentraci√≥n entre 50 y 70 a√±os.  

**HB (hemoglobina)**  
- Distribuci√≥n bastante sim√©trica.  
- Valores habituales entre 12 y 14 g/dL.  
- Valores extremos (<8 o >18) son poco frecuentes.  

**TLC (total leucocyte count)**  
- Alta asimetr√≠a positiva.  
- Mayor√≠a de valores en rangos bajos, pero algunos muy altos generan cola larga.  

**PLATELETS (plaquetas)**  
- Distribuci√≥n sesgada a la derecha.  
- Mayor densidad entre 200k y 300k, con casos aislados m√°s altos.  

**GLUCOSE (glucosa)**  
- Sesgo positivo pronunciado.  
- Pico en valores bajos, pero con casos >400.  

**UREA**  
- Fuerte asimetr√≠a positiva.  
- Mayor√≠a <100, pero con casos extremos elevados.  

**CREATININE (creatinina)**  
- Sesgo positivo fuerte.  
- Valores bajos dominan, pero hay casos altos dispersos.  

**EF (ejection fraction)**  
- Pico importante en 60.  
- Resto distribuido entre 20‚Äì40.  
- Genera una distribuci√≥n no sim√©trica con concentraci√≥n en el l√≠mite superior.  
""")

# ==========================
# RELACIONES BIVARIADAS
# ==========================
st.header("üìå Relaciones Bivariadas")

# -------------------------
# 1. Pairplot: Num√©ricas + G√©nero
# -------------------------
st.subheader("üìä Dispersi√≥n de variables num√©ricas por g√©nero")

st.markdown("""
El siguiente gr√°fico compara las variables num√©ricas contra la variable **G√âNERO**:

- **AGE**: No se observan tendencias lineales claras con otras variables.  
- **HB**: Ligera correlaci√≥n negativa con urea y creatinina; hombres tienden a valores m√°s altos.  
- **TLC**: Muy disperso, sin relaci√≥n marcada con otras variables.  
- **Plaquetas (PLATELETS)**: No muestra correlaciones fuertes, dispersi√≥n amplia en ambos g√©neros.  
- **Glucose vs Urea/Creatinine**: Casos con glucosa muy alta tienden a mostrar tambi√©n urea/creatinina altos.  
- **Urea y Creatinine**: Relaci√≥n positiva clara.  
- **EF (fracci√≥n de eyecci√≥n)**: Concentraci√≥n en valor 60, con ligera relaci√≥n inversa con urea/creatinina.  
""")

import seaborn as sns
pair_fig = sns.pairplot(df[num_features + ["GENDER"]], 
                        hue="GENDER", diag_kind="hist", height=2.5)
st.pyplot(pair_fig)


# ==========================
# HOSPITALIZACIONES POR SEXO
# ==========================
st.header("üë©‚Äçü¶∞üë® Hospitalizaciones por G√©nero")

gender_counts = df['GENDER'].value_counts().rename(index={1: 'Masculino', 0: 'Femenino'})

fig, ax = plt.subplots(figsize=(8,6))
sns.barplot(x=gender_counts.index, y=gender_counts.values, palette='viridis', ax=ax)
ax.set_title("Distribuci√≥n de G√©nero", fontsize=16)
ax.set_xlabel("G√©nero", fontsize=12)
ax.set_ylabel("Cantidad de Personas", fontsize=12)

for i, value in enumerate(gender_counts.values):
    ax.text(i, value, str(value), ha='center', va='bottom', fontsize=10)

st.pyplot(fig)

st.markdown("""
**Conclusi√≥n:**  
La mayor cantidad de pacientes corresponde al **g√©nero masculino**.
""")


# ==========================
# HOSPITALIZACIONES POR EDAD
# ==========================
st.header("üìÖ Hospitalizaciones seg√∫n Edad")

fig, ax = plt.subplots(figsize=(10,6))
sns.histplot(data=df, x="AGE", bins=20, kde=False, color="blue", ax=ax)
ax.set_title("Distribuci√≥n de Edades", fontsize=16)
ax.set_xlabel("Edad", fontsize=12)
ax.set_ylabel("Frecuencia", fontsize=12)
st.pyplot(fig)

st.markdown("""
**Conclusi√≥n:**

- **Pico de hospitalizaciones**: entre **55 y 63 a√±os**, seguido por 63‚Äì68 a√±os.  
- **Asimetr√≠a negativa**: la mayor concentraci√≥n de hospitalizaciones ocurre en personas mayores.  
- **Menor frecuencia**: en edades de **0 a 20 a√±os**, con hospitalizaciones m√°s asociadas a accidentes o condiciones cong√©nitas.  

En general, el **envejecimiento** se relaciona fuertemente con la mayor probabilidad de hospitalizaci√≥n debido a comorbilidades y deterioro natural de la salud.
""")

# ==========================
# EDAD vs D√çAS DE HOSPITALIZACI√ìN
# ==========================
st.header("üìå Relaci√≥n entre Edad y D√≠as de Hospitalizaci√≥n")

fig, ax = plt.subplots(figsize=(8,6))
sns.scatterplot(data=df, x="AGE", y="DURATION OF STAY", alpha=0.6, ax=ax)
sns.regplot(data=df, x="AGE", y="DURATION OF STAY", scatter=False, color="red", ax=ax)

ax.set_title("Relaci√≥n entre Edad y D√≠as de Hospitalizaci√≥n")
ax.set_xlabel("Edad del paciente")
ax.set_ylabel("D√≠as de hospitalizaci√≥n")
ax.grid(True, linestyle="--", alpha=0.5)
st.pyplot(fig)

# Correlaci√≥n num√©rica
corr = df["AGE"].corr(df["DURATION OF STAY"])
st.write(f"**Correlaci√≥n entre edad y d√≠as de hospitalizaci√≥n:** {corr:.4f}")

st.markdown("""
üìä **Interpretaci√≥n:**  
- Existe una correlaci√≥n **positiva muy d√©bil** (~0.106).  
- Aunque los pacientes de mayor edad tienden a permanecer un poco m√°s en el hospital, la relaci√≥n **no es fuerte**.  
- La gran dispersi√≥n de los puntos confirma que **otros factores cl√≠nicos y comorbilidades** influyen m√°s en la duraci√≥n de la hospitalizaci√≥n.  
""")


# ==========================
# 3. Divisi√≥n Train/Test
# ==========================
st.header("‚öôÔ∏è Divisi√≥n en Conjuntos de Entrenamiento y Prueba")

# Definimos variables predictoras y objetivo
X = df[num_features + cat_features]
y = df["DURATION OF STAY"]

# Divisi√≥n
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

st.write(f"Tama√±o del conjunto de entrenamiento: {X_train.shape[0]} registros")
st.write(f"Tama√±o del conjunto de prueba: {X_test.shape[0]} registros")

st.markdown("""
La variable objetivo es **DURATION OF STAY** (d√≠as de hospitalizaci√≥n).  
Su predicci√≥n es de gran valor para la **planificaci√≥n cl√≠nica y operativa**, permitiendo optimizar:  
- Disponibilidad de camas üõèÔ∏è  
- Asignaci√≥n de personal üë©‚Äç‚öïÔ∏èüë®‚Äç‚öïÔ∏è  
- Gesti√≥n de recursos hospitalarios ‚öïÔ∏è  
""")


# ==========================
# 2.2 Preprocesamiento
# ==========================
st.header("üîß Preprocesamiento de Datos")

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import RobustScaler

# Pipeline para num√©ricas
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="mean")),
    ("scaler", RobustScaler())
])

# Pipeline para categ√≥ricas
categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent"))
])

# ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, num_features),
        ("cat", categorical_transformer, cat_features)
    ]
)

# Aplicar
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

st.success("‚úÖ Preprocesamiento aplicado correctamente: imputaci√≥n y escalado realizados.")

# ==========================
# 4. Selecci√≥n de caracter√≠sticas: PCA
# ==========================
st.header("üß© Selecci√≥n de Caracter√≠sticas - PCA")

# 1. Extraer num√©ricas procesadas
num_indices = [i for i, col in enumerate(X_train.columns) if col in num_features]

X_train_numericas = pd.DataFrame(
    X_train_processed[:, num_indices],
    columns=num_features
)
X_test_numericas = pd.DataFrame(
    X_test_processed[:, num_indices],
    columns=num_features
)

# 2. PCA (90% var. explicada)
from sklearn.decomposition import PCA
pca = PCA(n_components=0.90, random_state=42)
Xn_train_pca = pca.fit_transform(X_train_numericas)
Xn_test_pca  = pca.transform(X_test_numericas)

pca_names = [f"PCA{i+1}" for i in range(Xn_train_pca.shape[1])]
Xn_train_pca = pd.DataFrame(Xn_train_pca, columns=pca_names, index=X_train.index)
Xn_test_pca  = pd.DataFrame(Xn_test_pca,  columns=pca_names, index=X_test.index)

st.success(f"PCA gener√≥ **{len(pca_names)} componentes**, con una varianza acumulada explicada de **{pca.explained_variance_ratio_.sum():.3f}**.")

# 3. Gr√°fico de varianza explicada
var_exp = pca.explained_variance_ratio_
cum_var_exp = np.cumsum(var_exp)

fig, ax = plt.subplots(figsize=(8,5))
ax.bar(range(1, len(var_exp)+1), var_exp, alpha=0.6, label="Varianza explicada por componente")
ax.step(range(1, len(cum_var_exp)+1), cum_var_exp, where="mid", color="red", label="Varianza acumulada")
ax.axhline(y=0.9, color="green", linestyle="--", label="90%")

ax.set_xlabel("Componentes principales")
ax.set_ylabel("Proporci√≥n de varianza explicada")
ax.set_title("PCA - Varianza explicada y acumulada")
ax.set_xticks(range(1, len(var_exp)+1))
ax.legend()
ax.grid(True)
st.pyplot(fig)

st.markdown("""
üìä **Interpretaci√≥n del PCA:**  
- Para las variables num√©ricas, se logran construir **3 componentes principales**.  
- Estas explican cerca del **74% de la varianza total**.  
- La **primera componente** concentra la mayor parte (~47%), mostrando que algunas variables dominan la variabilidad.  
""")


# 4. Gr√°fico de dispersi√≥n en las 2 primeras componentes
pc1 = Xn_train_pca.iloc[:, 0]
pc2 = Xn_train_pca.iloc[:, 1]

fig2, ax2 = plt.subplots(figsize=(10,8))
ax2.scatter(pc1, pc2, alpha=0.6, s=20)
ax2.set_xlabel(f"Componente Principal 1 ({pca.explained_variance_ratio_[0]*100:.2f}%)")
ax2.set_ylabel(f"Componente Principal 2 ({pca.explained_variance_ratio_[1]*100:.2f}%)")
ax2.set_title("Distribuci√≥n de los datos en el espacio PCA (2 primeras componentes)")
ax2.grid(True)
st.pyplot(fig2)

st.markdown("""
üîé **An√°lisis del gr√°fico PCA (2D):**  
- La **primera componente** explica un **43.31%** de la varianza.  
- La **segunda componente** explica un **16.28%**.  
- Los puntos cerca del **(0,0)** representan observaciones "t√≠picas".  
- Los puntos alejados del origen pueden ser **valores at√≠picos**.  
- La dispersi√≥n es mayor en la **componente 1**, lo que indica que esta concentra mayor variabilidad de la informaci√≥n.  
""")
# ==========================
# 4.1. PCA: Heatmap de Loadings
# ==========================
st.subheader("üìå PCA - Heatmap de Loadings")

# DataFrame con loadings
loadings = pd.DataFrame(pca.components_, columns=num_features, index=pca_names)

fig3, ax3 = plt.subplots(figsize=(12, 8))
sns.heatmap(loadings.T, annot=True, fmt=".2f", cmap="coolwarm", cbar=True, ax=ax3)
ax3.set_title("Mapa de calor de los loadings del PCA")
ax3.set_xlabel("Componentes principales")
ax3.set_ylabel("Variables originales")
st.pyplot(fig3)

st.markdown("""
**Interpretaci√≥n de los loadings:**  
- **PCA1**: fuertemente influenciada por `CREATININE (0.81)` y `UREA (0.54)`.  
  ‚Üí Relacionada con la **funci√≥n renal**.  
- **PCA2**: dominada por `TLC (0.96)`.  
  ‚Üí Relacionada con la **respuesta inmune / estado infeccioso**.  
- **PCA3**: fuerte correlaci√≥n con `GLUCOSE (0.92)` y moderada con `AGE (0.23)`.  
  ‚Üí Relacionada con **niveles de glucosa**.  
""")


# ==========================
# 4.2. MCA: An√°lisis de Correspondencias M√∫ltiples
# ==========================
st.subheader("üìå MCA - Variables Categ√≥ricas")

# Eigenvalues resumen (inercia por eje)
ev = mca.eigenvalues_summary.copy()

# Asegurar tipo num√©rico
ev["% of variance"] = ev["% of variance"].replace("%", "", regex=True)
ev["% of variance"] = ev["% of variance"].str.replace(",", ".", regex=False)
ev["% of variance"] = pd.to_numeric(ev["% of variance"], errors="coerce")

var_exp_mca = ev["% of variance"].values / 100
cum_var_exp_mca = np.cumsum(var_exp_mca)
componentes = np.arange(1, len(var_exp_mca) + 1)

# Gr√°fico Scree Plot
fig4, ax4 = plt.subplots(figsize=(8, 5))
ax4.bar(componentes, var_exp_mca, alpha=0.7, label="Varianza explicada")
ax4.plot(componentes, cum_var_exp_mca, marker="o", color="red", label="Varianza acumulada")
ax4.set_xticks(componentes)
ax4.set_xlabel("Componentes MCA")
ax4.set_ylabel("Proporci√≥n de varianza")
ax4.set_title("Scree plot - MCA")
ax4.legend()
ax4.grid(alpha=0.3)
st.pyplot(fig4)

st.markdown("""
üìä **Interpretaci√≥n MCA:**  
- Las **5 primeras dimensiones** solo explican alrededor del **25% de la varianza**.  
- Esto indica que las **relaciones entre categor√≠as son difusas** y no se logra una reducci√≥n de dimensionalidad tan clara como en PCA.  
""")


# ==========================
# 4.3. Heatmap MCA - Loadings
# ==========================
st.subheader("üìå MCA - Heatmap de Categor√≠as vs Componentes")

coords = mca.column_coordinates(X_train_categoricas)
coords.index = coords.index.astype(str)

# Filtrado por threshold (opcional)
threshold = 0.2
filtered_data = coords.loc[:, coords.abs().max() > threshold]

fig5, ax5 = plt.subplots(figsize=(10, 6))
sns.heatmap(filtered_data, cmap="coolwarm", center=0, ax=ax5)
ax5.set_title("Heatmap de loadings - MCA")
ax5.set_xlabel("Componentes")
ax5.set_ylabel("Categor√≠as")
st.pyplot(fig5)

st.markdown("""
üîé **Interpretaci√≥n MCA:**  
- **Dimensi√≥n 1**: relacionada con desenlaces (`OUTCOME_DAMA_1.0`, `SHOCK_0`)  
- **Dimensi√≥n 2**: relacionada con **factores de riesgo** (`ALCOHOL_1.0`, `HTN_1.0`, `CAD_1.0`).  
- **Dimensi√≥n 3**: asociada a condiciones **agudas** (`STEMI_1.0`, `ENDOCARDITIS`).  
- Dimensiones >3 muestran contribuciones m√°s difusas.  
""")


# ==========================
# 4.4. Scatterplot MCA
# ==========================
st.subheader("üìå MCA - Scatterplot (2 primeras componentes)")

row_coords = mca.row_coordinates(X_train_categoricas)

fig6, ax6 = plt.subplots(figsize=(10, 6))
sns.scatterplot(x=row_coords[0], y=row_coords[1], alpha=0.6, ax=ax6)

explained_variance_ratio = mca.eigenvalues_
ax6.set_xlabel(f"Componente 1 ({explained_variance_ratio[0]*100:.2f}%)")
ax6.set_ylabel(f"Componente 2 ({explained_variance_ratio[1]*100:.2f}%)")
ax6.set_title("MCA - Scatterplot de las dos primeras componentes")
ax6.grid(True, linestyle="--", alpha=0.5)
st.pyplot(fig6)

st.markdown("""
üìå **An√°lisis Scatterplot MCA:**  
- **Componente 1**: explica aprox. **8.06%**.  
- **Componente 2**: explica aprox. **4.89%**.  
- La varianza explicada es baja ‚Üí las 2D no capturan gran parte de la informaci√≥n.  
- No se aprecian **clusters claros** ‚Üí las categor√≠as est√°n dispersas.  
""")

# ==========================
# 4.5. Dataset reducido
# ==========================
X_train_reduced = pd.concat([Xn_train_pca, Xc_train_mca], axis=1)
X_test_reduced  = pd.concat([Xn_test_pca,  Xc_test_mca],  axis=1)

st.success(f"‚úÖ Shape train reducido: {X_train_reduced.shape}, Shape test reducido: {X_test_reduced.shape}")
