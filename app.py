# ============================================
# Paso 1 ‚Äî Librer√≠as y configuraci√≥n Streamlit
# ============================================
import streamlit as st

st.set_page_config(page_title="Proyecto ML - Librer√≠as y Setup", layout="wide")

# --- Librer√≠as base
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# --- Estad√≠stica / ML
from scipy.stats import spearmanr, stats
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestRegressor

# --- prince (MCA) con manejo de ausencia
try:
    import prince
    HAS_PRINCE = True
except Exception:
    HAS_PRINCE = False

st.title("Proyecto ML ‚Äî Setup de Librer√≠as")
st.markdown("Este m√≥dulo adapta la celda de **carga de librer√≠as** a un entorno Streamlit.")

# Estado de dependencias
col1, col2 = st.columns(2)
with col1:
    st.success("‚úÖ pandas, numpy, matplotlib, scipy, scikit-learn disponibles")
with col2:
    if HAS_PRINCE:
        st.success("‚úÖ `prince` instalado (MCA habilitado)")
    else:
        st.warning(
            "‚ÑπÔ∏è `prince` no est√° instalado. "
            "Para habilitar MCA, agrega `prince` a tu `requirements.txt` "
            "o instala localmente con `pip install prince`."
        )

with st.expander("Descripci√≥n de la base de datos", expanded=True):
    st.markdown("""
Este conjunto de datos corresponde a los registros de **14.845 admisiones hospitalarias** 
(**12.238** pacientes, incluyendo **1.921** con m√∫ltiples ingresos) recogidos durante un per√≠odo de dos a√±os 
(**1 de abril de 2017** a **31 de marzo de 2019**) en el **Hero DMC Heart Institute**, unidad del 
**Dayanand Medical College and Hospital** en **Ludhiana, Punjab, India**.

**La informaci√≥n incluye:**

- **Datos demogr√°ficos:** edad, g√©nero y procedencia (rural o urbana).
- **Detalles de admisi√≥n:** tipo de admisi√≥n (emergencia u OPD), fechas de ingreso y alta, 
  duraci√≥n total de la estancia y **duraci√≥n en UCI** *(columna objetivo en este proyecto)*.
- **Antecedentes m√©dicos:** tabaquismo, consumo de alcohol, diabetes mellitus (DM), hipertensi√≥n (HTN),
  enfermedad arterial coronaria (CAD), cardiomiopat√≠a previa (CMP) y enfermedad renal cr√≥nica (CKD).
- **Par√°metros de laboratorio:** hemoglobina (HB), conteo total de leucocitos (TLC), plaquetas, glucosa, 
  urea, creatinina, p√©ptido natriur√©tico cerebral (BNP), enzimas card√≠acas elevadas (RCE) y fracci√≥n de eyecci√≥n (EF).
- **Condiciones cl√≠nicas y comorbilidades:** m√°s de 28 variables como insuficiencia card√≠aca, infarto con elevaci√≥n del ST (STEMI),
  embolia pulmonar, shock, infecciones respiratorias, entre otras.
- **Resultado hospitalario:** estado al alta (alta m√©dica o fallecimiento).
    """)

# ================================
# Diccionario de variables (UI)
# ================================
with st.expander("Diccionario de variables", expanded=True):
    data = [
        {"Nombre de la variable":"SNO","Nombre completo":"Serial Number","Explicaci√≥n breve":"N√∫mero √∫nico de registro"},
        {"Nombre de la variable":"MRD No.","Nombre completo":"Admission Number","Explicaci√≥n breve":"N√∫mero asignado al ingreso"},
        {"Nombre de la variable":"D.O.A","Nombre completo":"Date of Admission","Explicaci√≥n breve":"Fecha en que el paciente fue admitido"},
        {"Nombre de la variable":"D.O.D","Nombre completo":"Date of Discharge","Explicaci√≥n breve":"Fecha en que el paciente fue dado de alta"},
        {"Nombre de la variable":"AGE","Nombre completo":"AGE","Explicaci√≥n breve":"Edad del paciente"},
        {"Nombre de la variable":"GENDER","Nombre completo":"GENDER","Explicaci√≥n breve":"Sexo del paciente"},
        {"Nombre de la variable":"RURAL","Nombre completo":"RURAL(R) /Urban(U)","Explicaci√≥n breve":"Zona de residencia (rural/urbana)"},
        {"Nombre de la variable":"TYPE OF ADMISSION-EMERGENCY/OPD","Nombre completo":"TYPE OF ADMISSION-EMERGENCY/OPD","Explicaci√≥n breve":"Si el ingreso fue por urgencias o consulta externa"},
        {"Nombre de la variable":"month year","Nombre completo":"month year","Explicaci√≥n breve":"Mes y a√±o del ingreso"},
        {"Nombre de la variable":"DURATION OF STAY","Nombre completo":"DURATION OF STAY","Explicaci√≥n breve":"D√≠as totales de hospitalizaci√≥n"},
        {"Nombre de la variable":"duration of intensive unit stay","Nombre completo":"duration of intensive unit stay","Explicaci√≥n breve":"Duraci√≥n de la estancia en UCI"},
        {"Nombre de la variable":"OUTCOME","Nombre completo":"OUTCOME","Explicaci√≥n breve":"Resultado del paciente (alta, fallecimiento, etc.)"},
        {"Nombre de la variable":"SMOKING","Nombre completo":"SMOKING","Explicaci√≥n breve":"Historial de consumo de tabaco"},
        {"Nombre de la variable":"ALCOHOL","Nombre completo":"ALCOHOL","Explicaci√≥n breve":"Historial de consumo de alcohol"},
        {"Nombre de la variable":"DM","Nombre completo":"Diabetes Mellitus","Explicaci√≥n breve":"Diagn√≥stico de diabetes mellitus"},
        {"Nombre de la variable":"HTN","Nombre completo":"Hypertension","Explicaci√≥n breve":"Diagn√≥stico de hipertensi√≥n arterial"},
        {"Nombre de la variable":"CAD","Nombre completo":"Coronary Artery Disease","Explicaci√≥n breve":"Diagn√≥stico de enfermedad coronaria"},
        {"Nombre de la variable":"PRIOR CMP","Nombre completo":"CARDIOMYOPATHY","Explicaci√≥n breve":"Historial de miocardiopat√≠a"},
        {"Nombre de la variable":"CKD","Nombre completo":"CHRONIC KIDNEY DISEASE","Explicaci√≥n breve":"Diagn√≥stico de enfermedad renal cr√≥nica"},
        {"Nombre de la variable":"HB","Nombre completo":"Haemoglobin","Explicaci√≥n breve":"Nivel de hemoglobina en sangre"},
        {"Nombre de la variable":"TLC","Nombre completo":"TOTAL LEUKOCYTES COUNT","Explicaci√≥n breve":"Conteo total de leucocitos"},
        {"Nombre de la variable":"PLATELETS","Nombre completo":"PLATELETS","Explicaci√≥n breve":"Conteo de plaquetas"},
        {"Nombre de la variable":"GLUCOSE","Nombre completo":"GLUCOSE","Explicaci√≥n breve":"Nivel de glucosa en sangre"},
        {"Nombre de la variable":"UREA","Nombre completo":"UREA","Explicaci√≥n breve":"Nivel de urea en sangre"},
        {"Nombre de la variable":"CREATININE","Nombre completo":"CREATININE","Explicaci√≥n breve":"Nivel de creatinina en sangre"},
        {"Nombre de la variable":"BNP","Nombre completo":"B-TYPE NATRIURETIC PEPTIDE","Explicaci√≥n breve":"P√©ptido relacionado con funci√≥n card√≠aca"},
        {"Nombre de la variable":"RAISED CARDIAC ENZYMES","Nombre completo":"RAISED CARDIAC ENZYMES","Explicaci√≥n breve":"Presencia de enzimas card√≠acas elevadas"},
        {"Nombre de la variable":"EF","Nombre completo":"Ejection Fraction","Explicaci√≥n breve":"Fracci√≥n de eyecci√≥n card√≠aca"},
        {"Nombre de la variable":"SEVERE ANAEMIA","Nombre completo":"SEVERE ANAEMIA","Explicaci√≥n breve":"Presencia de anemia grave"},
        {"Nombre de la variable":"ANAEMIA","Nombre completo":"ANAEMIA","Explicaci√≥n breve":"Presencia de anemia"},
        {"Nombre de la variable":"STABLE ANGINA","Nombre completo":"STABLE ANGINA","Explicaci√≥n breve":"Dolor tor√°cico estable por angina"},
        {"Nombre de la variable":"ACS","Nombre completo":"Acute coronary Syndrome","Explicaci√≥n breve":"S√≠ndrome coronario agudo"},
        {"Nombre de la variable":"STEMI","Nombre completo":"ST ELEVATION MYOCARDIAL INFARCTION","Explicaci√≥n breve":"Infarto agudo de miocardio con elevaci√≥n del ST"},
        {"Nombre de la variable":"ATYPICAL CHEST PAIN","Nombre completo":"ATYPICAL CHEST PAIN","Explicaci√≥n breve":"Dolor tor√°cico no t√≠pico"},
        {"Nombre de la variable":"HEART FAILURE","Nombre completo":"HEART FAILURE","Explicaci√≥n breve":"Diagn√≥stico de insuficiencia card√≠aca"},
        {"Nombre de la variable":"HFREF","Nombre completo":"HEART FAILURE WITH REDUCED EJECTION FRACTION","Explicaci√≥n breve":"Insuficiencia card√≠aca con fracci√≥n de eyecci√≥n reducida"},
        {"Nombre de la variable":"HFNEF","Nombre completo":"HEART FAILURE WITH NORMAL EJECTION FRACTION","Explicaci√≥n breve":"Insuficiencia card√≠aca con fracci√≥n de eyecci√≥n conservada"},
        {"Nombre de la variable":"VALVULAR","Nombre completo":"Valvular Heart Disease","Explicaci√≥n breve":"Enfermedad de v√°lvulas card√≠acas"},
        {"Nombre de la variable":"CHB","Nombre completo":"Complete Heart Block","Explicaci√≥n breve":"Bloqueo card√≠aco completo"},
        {"Nombre de la variable":"SSS","Nombre completo":"Sick sinus syndrome","Explicaci√≥n breve":"S√≠ndrome de disfunci√≥n sinusal"},
        {"Nombre de la variable":"AKI","Nombre completo":"ACUTE KIDNEY INJURY","Explicaci√≥n breve":"Lesi√≥n renal aguda"},
        {"Nombre de la variable":"CVA INFRACT","Nombre completo":"Cerebrovascular Accident INFRACT","Explicaci√≥n breve":"Accidente cerebrovascular isqu√©mico"},
        {"Nombre de la variable":"CVA BLEED","Nombre completo":"Cerebrovascular Accident BLEED","Explicaci√≥n breve":"Accidente cerebrovascular hemorr√°gico"},
        {"Nombre de la variable":"AF","Nombre completo":"Atrial Fibrilation","Explicaci√≥n breve":"Fibrilaci√≥n auricular"},
        {"Nombre de la variable":"VT","Nombre completo":"Ventricular Tachycardia","Explicaci√≥n breve":"Taquicardia ventricular"},
        {"Nombre de la variable":"PSVT","Nombre completo":"PAROXYSMAL SUPRA VENTRICULAR TACHYCARDIA","Explicaci√≥n breve":"Taquicardia supraventricular parox√≠stica"},
        {"Nombre de la variable":"CONGENITAL","Nombre completo":"Congenital Heart Disease","Explicaci√≥n breve":"Enfermedad card√≠aca cong√©nita"},
        {"Nombre de la variable":"UTI","Nombre completo":"Urinary tract infection","Explicaci√≥n breve":"Infecci√≥n de v√≠as urinarias"},
        {"Nombre de la variable":"NEURO CARDIOGENIC SYNCOPE","Nombre completo":"NEURO CARDIOGENIC SYNCOPE","Explicaci√≥n breve":"S√≠ncope de origen cardiog√©nico"},
        {"Nombre de la variable":"ORTHOSTATIC","Nombre completo":"ORTHOSTATIC","Explicaci√≥n breve":"Hipotensi√≥n postural"},
        {"Nombre de la variable":"INFECTIVE ENDOCARDITIS","Nombre completo":"INFECTIVE ENDOCARDITIS","Explicaci√≥n breve":"Inflamaci√≥n de las v√°lvulas card√≠acas por infecci√≥n"},
        {"Nombre de la variable":"DVT","Nombre completo":"Deep venous thrombosis","Explicaci√≥n breve":"Trombosis venosa profunda"},
        {"Nombre de la variable":"CARDIOGENIC SHOCK","Nombre completo":"CARDIOGENIC SHOCK","Explicaci√≥n breve":"Shock de origen card√≠aco"},
        {"Nombre de la variable":"SHOCK","Nombre completo":"SHOCK","Explicaci√≥n breve":"Shock por otras causas"},
        {"Nombre de la variable":"PULMONARY EMBOLISM","Nombre completo":"PULMONARY EMBOLISM","Explicaci√≥n breve":"Bloqueo de arterias pulmonares por co√°gulo"},
        {"Nombre de la variable":"CHEST INFECTION","Nombre completo":"CHEST INFECTION","Explicaci√≥n breve":"Infecci√≥n pulmonar"},
        {"Nombre de la variable":"DAMA","Nombre completo":"Discharged Against Medical Advice","Explicaci√≥n breve":"Alta m√©dica solicitada por el paciente en contra de la recomendaci√≥n"},
    ]

    dicc_df = pd.DataFrame(data, columns=["Nombre de la variable","Nombre completo","Explicaci√≥n breve"])

    # Buscador simple
    q = st.text_input("Buscar en el diccionario‚Ä¶")
    if q:
        mask = dicc_df.apply(lambda r: r.astype(str).str.contains(q, case=False, na=False).any(), axis=1)
        st.dataframe(dicc_df[mask], use_container_width=True)
    else:
        st.dataframe(dicc_df, use_container_width=True)

    # Descargar CSV
    csv_bytes = dicc_df.to_csv(index=False).encode("utf-8-sig")
    st.download_button("Descargar diccionario (CSV)", data=csv_bytes,
                       file_name="diccionario_variables.csv", mime="text/csv")
import pandas as pd

url = "https://raw.githubusercontent.com/Juansebastianrde/Reduccion-de-dimensionalidad/main/HDHI%20Admission%20data.csv"
df = pd.read_csv(url, sep=None, engine="python")  # infiere el separador

st.header("1. Cargar base de datos")

# Lee el CSV desde el archivo local (misma carpeta que la app)
bd = pd.read_csv("HDHI Admission data.csv", sep=None, engine="python")  # infiere el separador
st.success(f"Datos cargados: {bd.shape}")

# Equivalente a bd.head()
st.dataframe(bd.head(5), use_container_width=True)


st.subheader("Resumen de columnas (nulos y dtypes)")

# bd: tu DataFrame ya cargado
summary = pd.DataFrame({
    "dtype": bd.dtypes.astype(str),
    "non_null": bd.notna().sum(),
    "nulls": bd.isna().sum(),
    "unique": bd.nunique(dropna=True)
})
summary["%nulls"] = (summary["nulls"] / len(bd) * 100).round(2)

# Orden sugerido de columnas
summary = summary[["dtype", "non_null", "nulls", "%nulls", "unique"]]

st.caption(f"Filas: {len(bd):,}")
st.dataframe(summary, use_container_width=True)

st.subheader("Eliminar columna: BNP")
st.markdown("**Se decide eliminar la variabla BNP dado que tiene m√°s del 50% de valores faltantes.**")

if "BNP" in bd.columns:
    bd.drop("BNP", axis=1, inplace=True)
    st.success("Columna 'BNP' eliminada.")
else:
    st.info("La columna 'BNP' no existe en el DataFrame.")


st.header("2. Eliminar variables y transformar datos")

# Partimos del DataFrame cargado previamente: `bd`
st.write("Shape inicial:", bd.shape)

# ==============================
# Eliminar variables innecesarias
# ==============================
st.markdown("**Eliminar variables innecesarias**")
cols_drop = ["SNO", "MRD No.", "month year"]
present = [c for c in cols_drop if c in bd.columns]
df = bd.drop(columns=present, errors="ignore").copy()
st.success(f"Columnas eliminadas: {', '.join(present) if present else 'ninguna (no se encontraron)'}")

# =======================================
# Transformar variables de fecha a datetime
# =======================================
st.markdown("**Transformar variables de fecha a formato datetime**")
if "D.O.A" in df.columns:
    df["D.O.A"] = pd.to_datetime(df["D.O.A"], format="%m/%d/%Y", errors="coerce")
if "D.O.D" in df.columns:
    df["D.O.D"] = pd.to_datetime(df["D.O.D"], format="%m/%d/%Y", errors="coerce")

# ===========================================================
# Limpiar num√©ricas que vienen como texto y convertir a n√∫mero
# ===========================================================
st.markdown("**Tratamiento de variables num√©ricas mal tipadas**")
cols_to_clean = ["HB", "TLC", "PLATELETS", "GLUCOSE", "UREA", "CREATININE", "EF"]
cols_found = [c for c in cols_to_clean if c in df.columns]

for col in cols_found:
    df[col] = (
        df[col]
        .astype(str)  # asegurar string
        .str.strip()
        .replace(["EMPTY", "nan", "NaN", "None", ""], np.nan)  # a NaN
        .str.replace(r"[<>]", "", regex=True)  # quitar > y <
        .str.replace(",", ".", regex=False)    # coma decimal -> punto
    )
for col in cols_found:
    df[col] = pd.to_numeric(df[col], errors="coerce")

st.success(f"Columnas limpiadas y convertidas a num√©rico: {', '.join(cols_found) if cols_found else 'ninguna'}")

import streamlit as st
import pandas as pd

st.subheader("Mapeo a 0/1 y dummies")

# Toma el df ya trabajado (o usa df local si lo tienes en el scope)
df = st.session_state.get("df", df).copy()

col_adm = "TYPE OF ADMISSION-EMERGENCY/OPD"
modificadas = []

# GENDER: M/F -> 1/0
if "GENDER" in df.columns:
    df["GENDER"] = (
        df["GENDER"].astype(str).str.strip().str.upper().map({"M": 1, "F": 0})
    )
    modificadas.append("GENDER")

# RURAL: R/U -> 1/0
if "RURAL" in df.columns:
    df["RURAL"] = (
        df["RURAL"].astype(str).str.strip().str.upper().map({"R": 1, "U": 0})
    )
    modificadas.append("RURAL")

# TYPE OF ADMISSION-EMERGENCY/OPD: E/O -> 1/0
if col_adm in df.columns:
    df[col_adm] = (
        df[col_adm].astype(str).str.strip().str.upper().map({"E": 1, "O": 0})
    )
    modificadas.append(col_adm)

# CHEST INFECTION: '1'/'0' o 1/0 -> 1/0
if "CHEST INFECTION" in df.columns:
    # convierte cualquier cosa a num√©rico; strings inv√°lidos -> NaN
    df["CHEST INFECTION"] = pd.to_numeric(df["CHEST INFECTION"], errors="coerce").astype("Int64")
    modificadas.append("CHEST INFECTION")

# OUTCOME -> dummies (mant√©n todas las categor√≠as)
if "OUTCOME" in df.columns:
    df = pd.get_dummies(df, columns=["OUTCOME"], drop_first=False, dtype=int)
    modificadas.append("OUTCOME (dummies)")

# Booleans -> 0/1
bool_cols = df.select_dtypes(include=bool).columns
if len(bool_cols) > 0:
    df[bool_cols] = df[bool_cols].astype(int)

# Guarda y muestra
st.session_state["df"] = df
st.success(f"Columnas mapeadas: {', '.join(modificadas) if modificadas else '‚Äî'}")

cols_preview = [c for c in ["GENDER", "RURAL", col_adm, "CHEST INFECTION"] if c in df.columns]
cols_preview += [c for c in df.columns if c.startswith("OUTCOME_")]
if cols_preview:
    st.dataframe(df[cols_preview].head(), use_container_width=True)
else:
    st.info("No se encontraron columnas para mostrar en el preview.")


# ==========================================
# Convertir columnas booleanas a 0/1 (int)
# ==========================================
bool_cols = df.select_dtypes(include=bool).columns
if len(bool_cols) > 0:
    df[bool_cols] = df[bool_cols].astype(int)
    st.success(f"Booleans convertidos a 0/1: {', '.join(bool_cols)}")
else:
    st.info("No se encontraron columnas booleanas para convertir.")

st.subheader("Decisi√≥n sobre variable de UCI")
st.markdown("""
**Teniendo en cuenta que la variable que se refiere a duraci√≥n en la unidad de cuidados intensivos contiene informaci√≥n que no se tiene cuando un paciente es ingresado al hospital, se decide eliminar con el objetivo de hacer un an√°lisis m√°s realista.**
""")

st.subheader("Eliminar variable de UCI")
col_uci = "duration of intensive unit stay"

if col_uci in df.columns:
    df.drop(col_uci, axis=1, inplace=True)
    st.success(f"Columna '{col_uci}' eliminada. Shape actual: {df.shape}")
else:
    st.info(f"La columna '{col_uci}' no existe en el DataFrame.")

st.subheader("Normalizar nombres de columnas (strip)")

# Ver columnas antes
cols_before = list(df.columns)
st.markdown("**Antes:**")
st.code("\n".join([str(c) for c in cols_before]))

# Aplicar strip
df.columns = df.columns.str.strip()

# Ver columnas despu√©s
cols_after = list(df.columns)
st.markdown("**Despu√©s:**")
st.code("\n".join([str(c) for c in cols_after]))

# Mostrar lista final en tabla simple
st.markdown("**Lista de columnas actual:**")
st.dataframe(pd.DataFrame({"columnas": cols_after}), use_container_width=True)

# Opcional: mostrar cu√°les cambiaron
changed = [i for i, (a, b) in enumerate(zip(cols_before, cols_after)) if a != b]
if changed:
    st.success(f"Columnas modificadas (√≠ndices): {changed}")
else:
    st.info("No hubo cambios en los nombres de columnas.")

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# FIX #1: Guardar el DF limpio para que TODO el resto lo use
st.session_state["df"] = df
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

st.subheader("2.1 Separaci√≥n en variables categ√≥ricas y variables num√©ricas")

# Lista base (tal como la definiste)
raw_cat_features = [
    'GENDER', 'RURAL', 'TYPE OF ADMISSION-EMERGENCY/OPD',
    'OUTCOME_DAMA', 'OUTCOME_DISCHARGE', 'OUTCOME_EXPIRY',
    'SMOKING', 'ALCOHOL', 'DM', 'HTN', 'CAD', 'PRIOR CMP', 'CKD',
    'RAISED CARDIAC ENZYMES', 'SEVERE ANAEMIA', 'ANAEMIA', 'STABLE ANGINA',
    'ACS', 'STEMI', 'ATYPICAL CHEST PAIN', 'HEART FAILURE', 'HFREF', 'HFNEF',
    'VALVULAR', 'CHB', 'SSS', 'AKI', 'CVA INFRACT', 'CVA BLEED', 'AF', 'VT', 'PSVT',
    'CONGENITAL', 'UTI', 'NEURO CARDIOGENIC SYNCOPE', 'ORTHOSTATIC',
    'INFECTIVE ENDOCARDITIS', 'DVT', 'CARDIOGENIC SHOCK', 'SHOCK',
    'PULMONARY EMBOLISM', 'CHEST INFECTION'
]

# Intersecci√≥n con columnas reales del DF para evitar errores
cat_features = [c for c in raw_cat_features if c in df.columns]

# Columnas a excluir del set num√©rico (fechas y target si existe)
exclude = [c for c in ['D.O.A', 'D.O.D', 'DURATION OF STAY'] if c in df.columns]

# Num√©ricas = todo lo que no sea categ√≥rica ni excluido
num_features = [c for c in df.columns if c not in cat_features + exclude]

# Feedback visual
st.success(f"Categ√≥ricas detectadas: {len(cat_features)} | Num√©ricas detectadas: {len(num_features)}")

c1, c2 = st.columns(2)
with c1:
    st.markdown("**Categ√≥ricas**")
    st.write(cat_features)
with c2:
    st.markdown("**Num√©ricas**")
    st.write(num_features)

# Guardar en sesi√≥n para reutilizar despu√©s
st.session_state["cat_features"] = cat_features
st.session_state["num_features"] = num_features

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import math

st.subheader("Boxplots de variables num√©ricas")

# Usa df procesado si lo guardaste en la sesi√≥n; si no, usa df
df_plot = st.session_state.get("df", df)

# Detecta columnas num√©ricas (o usa las que guardaste antes)
num_cols_default = st.session_state.get("num_features") or df_plot.select_dtypes(include=[np.number]).columns.tolist()

# Selecci√≥n de columnas a graficar (m√°ximo 16 por rejilla como en tu ejemplo 4x4)
cols_sel = st.multiselect(
    "Selecciona variables num√©ricas",
    options=num_cols_default,
    default=num_cols_default[:16]
)

if len(cols_sel) == 0:
    st.info("Selecciona al menos una columna para graficar.")
else:
    n = len(cols_sel)
    cols_per_row = 4
    rows = math.ceil(n / cols_per_row)

    fig, axes = plt.subplots(rows, cols_per_row, figsize=(20, 4.5 * rows))
    # Asegurar vector 1D de ejes aunque rows/cols cambien
    if isinstance(axes, np.ndarray):
        axes = axes.flatten()
    else:
        axes = np.array([axes])

    for i, col in enumerate(cols_sel):
        sns.boxplot(x=df_plot[col], ax=axes[i])
        axes[i].set_title(col)
        axes[i].tick_params(axis="x", labelrotation=45)

    # Eliminar ejes vac√≠os si sobran
    for j in range(i + 1, len(axes)):
        fig.delaxes(axes[j])

    fig.tight_layout()
    st.pyplot(fig)


st.subheader("2.2 Porcentaje de datos at√≠picos (m√©todo IQR)")
st.markdown("""
En las gr√°ficas anteriores se identific√≥ que varias variables num√©ricas presentan muchos at√≠picos.
A continuaci√≥n se calcula el **porcentaje de outliers** por variable usando el criterio **1.5 ¬∑ IQR**.
""")

st.subheader("2.3 Resumen de outliers por IQR (1.5¬∑IQR)")
# --- OUTLIERS POR IQR (1.5¬∑IQR) ---

df_use = st.session_state.get("df", df)
num_feats = st.session_state.get("num_features", num_features)

outliers_list = []
for c in num_feats:
    # Fuerza la columna a num√©rico (si hay strings -> NaN)
    col_num = pd.to_numeric(df_use[c], errors="coerce")
    s = col_num.dropna()
    if s.empty:
        continue

    Q1 = s.quantile(0.25)
    Q3 = s.quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR

    # M√°scara sobre la columna ya convertida a num√©rico
    mask = (col_num < lower) | (col_num > upper)

    temp = (
        df_use.loc[mask, [c]]
        .rename(columns={c: "value"})
        .assign(
            variable=c,
            lower_bound=lower,
            upper_bound=upper,
            row_index=lambda x: x.index
        )
    )
    outliers_list.append(temp)

if len(outliers_list) == 0:
    st.info("No se encontraron outliers con el criterio 1.5¬∑IQR.")
else:
    outliers = pd.concat(outliers_list, ignore_index=True)
    resumen = outliers.groupby("variable").size().reset_index(name="n_outliers")
    st.dataframe(resumen.sort_values("n_outliers", ascending=False), use_container_width=True)

    # % de outliers (usa df_use por si cambi√≥ el df)
    st.subheader("Porcentaje de outliers por variable")
    resumen["pct_outliers"] = (resumen["n_outliers"] / len(df_use) * 100).round(2)
    resumen_show = resumen.sort_values("pct_outliers", ascending=False).copy()
    resumen_show["pct_outliers"] = resumen_show["pct_outliers"].map(lambda x: f"{x:.2f}%")
    st.dataframe(resumen_show, use_container_width=True)

import streamlit as st
import pandas as pd
import numpy as np

st.subheader("Asimetr√≠a (skewness) de variables num√©ricas")

# Usa el DF procesado si est√° en sesi√≥n; si no, usa df
df_use = st.session_state.get("df", df)

# Toma las num√©ricas conocidas o detecta autom√°ticamente
num_cols = st.session_state.get("num_features") or df_use.select_dtypes(include=[np.number]).columns.tolist()
if not num_cols:
    st.info("No se detectaron variables num√©ricas.")
else:
    df_num = df_use[num_cols]

    # 1) Asimetr√≠a con pandas
    skew_series = df_num.skew(numeric_only=True).sort_values(ascending=False)
    skew_df = skew_series.to_frame(name="skew")
    skew_df["abs_skew"] = skew_df["skew"].abs()
    skew_df = skew_df.reset_index().rename(columns={"index": "variable"})

    st.markdown("**Asimetr√≠a con pandas:**")
    st.dataframe(skew_df, use_container_width=True)

    # 2) Variables con fuerte asimetr√≠a (|skew| > 2)
    st.markdown("**Variables con |asimetr√≠a| > 2:**")
    highly_skewed = skew_df[skew_df["abs_skew"] > 2].sort_values("abs_skew", ascending=False)

    if highly_skewed.empty:
        st.success("No hay variables con |asimetr√≠a| > 2.")
    else:
        st.dataframe(highly_skewed, use_container_width=True)

import streamlit as st
import numpy as np
import matplotlib.pyplot as plt

st.subheader("Histogramas de variables num√©ricas")

df_use = st.session_state.get("df", df)
num_feats = st.session_state.get("num_features") or df_use.select_dtypes(np.number).columns.tolist()

var_hist = st.multiselect("Elige variables", options=num_feats, default=num_feats)
bins = st.slider("Bins", 5, 100, 50, 5)

if var_hist:
    # NO pre-crear fig: deja que .hist cree la suya
    df_use[var_hist].hist(bins=bins, figsize=(12, 8))
    fig = plt.gcf()                 # captura la figura actual creada por .hist
    st.pyplot(fig)
    plt.close(fig)
else:
    st.info("Selecciona al menos una variable.")

st.subheader("An√°lisis univariado ‚Äî distribuci√≥n de variables")

with st.expander("Resumen interpretativo", expanded=True):
    st.markdown("""
**AGE (edad)**  
- La variable AGE (edad) presenta una distribuci√≥n aproximadamente normal con ligera asimetr√≠a hacia la derecha. La mayor parte de los registros se concentra entre los 50 y 70 a√±os, lo que refleja que la poblaci√≥n del dataset corresponde principalmente a adultos de mediana y mayor edad.

**HB (hemoglobina)**  
- La variable HB (hemoglobina) muestra una distribuci√≥n bastante sim√©trica, con la mayor densidad de valores entre 12 y 14 g/dL. Los valores extremos por debajo de 8 g/dL o por encima de 18 g/dL son poco frecuentes, lo que indica que la mayor√≠a de los registros se ubica en un rango considerado habitual.

**TLC (total leucocyte count)**  
- La variable TLC presenta una distribuci√≥n altamente asim√©trica a la derecha. La mayor√≠a de los valores se concentra en rangos bajos, mientras que existe un n√∫mero reducido de observaciones con valores muy elevados, que generan una cola larga en la distribuci√≥n.

**PLATELETS (plaquetas)**  
- La variable PLATELETS tiene una distribuci√≥n sesgada positivamente. La mayor concentraci√≥n se encuentra entre 200,000 y 300,000, aunque se observan registros con valores m√°s altos que extienden la cola de la distribuci√≥n.

**GLUCOSE (glucosa)**  
- La variable GLUCOSE muestra una distribuci√≥n asim√©trica hacia la derecha, con un pico en los valores bajos y una dispersi√≥n amplia que incluye observaciones por encima de 400. Esto evidencia la presencia de valores extremos elevados en el dataset.

**UREA**  
- La variable UREA presenta una fuerte asimetr√≠a positiva. La mayor√≠a de los valores se concentra por debajo de 100, aunque se registran observaciones con valores mucho m√°s altos, que extienden la distribuci√≥n hacia la derecha.

**CREATININE (creatinina)**  
- La variable CREATININE tambi√©n exhibe una asimetr√≠a positiva pronunciada. La mayor parte de los registros se concentra en valores bajos, mientras que existen observaciones dispersas con valores m√°s altos que alargan la cola de la distribuci√≥n.

**EF (ejection fraction)**  
- La variable EF (fracci√≥n de eyecci√≥n) muestra un patr√≥n particular: existe una concentraci√≥n importante de registros en el valor 60, mientras que el resto de la distribuci√≥n se reparte entre valores de 20 a 40. Esto genera una distribuci√≥n no sim√©trica con un pico muy marcado en el l√≠mite superior.
""")

st.subheader("Pairplot por g√©nero")

df_use = st.session_state.get("df", df)
num_feats_all = st.session_state.get("num_features") or df_use.select_dtypes(include=[np.number]).columns.tolist()

if "GENDER" not in df_use.columns:
    st.warning("No existe la columna 'GENDER' en el DataFrame.")
else:
    # Convertir a etiqueta si est√° en 0/1 (opcional)
    if set(df_use["GENDER"].dropna().unique()).issubset({0, 1}):
        hue_series = df_use["GENDER"].map({1: "M", 0: "F"})
        df_plot = df_use.copy()
        df_plot["GENDER"] = hue_series
    else:
        df_plot = df_use

    # Selecci√≥n de variables num√©ricas para el pairplot
    sel = st.multiselect(
        "Selecciona variables num√©ricas (m√°x. 6 recomendado)",
        options=list(num_feats_all),
        default=list(num_feats_all)[:4]
    )

    if len(sel) < 2:
        st.info("Elige al menos 2 variables para el pairplot.")
    else:
        cols = sel + ["GENDER"]
        grid = sns.pairplot(df_plot[cols].dropna(), hue="GENDER", diag_kind="hist", height=2.5)
        st.pyplot(grid.fig)
        plt.close(grid.fig)

st.header("Relaciones bivariadas")

with st.expander("Hallazgos principales", expanded=True):
    st.markdown("""
### Edad vs otras variables
- No se observan tendencias lineales marcadas entre **AGE** y las dem√°s variables.
- Los puntos est√°n bastante dispersos en ambos g√©neros.

### HB vs otras variables
- Ligera correlaci√≥n negativa con **Urea** y **Creatinine** (a medida que aumentan, la hemoglobina tiende a ser m√°s baja).
- Diferencia por g√©nero: los hombres concentran valores algo m√°s altos de **HB** en todos los rangos.

### TLC vs otras variables
- **TLC** presenta gran dispersi√≥n, con muchos valores extremos, pero no muestra relaci√≥n clara con otras variables.
- La distribuci√≥n por g√©nero es muy similar.

### Plaquetas (PLATELETS)
- No se aprecian correlaciones fuertes con otras variables.
- La dispersi√≥n es amplia y comparable entre hombres y mujeres.

### Glucose vs Urea/Creatinine
- No hay una correlaci√≥n directa clara, aunque algunos casos con **glucosa** muy alta tambi√©n muestran valores elevados de **urea** o **creatinina**.
- Ambos g√©neros siguen el mismo patr√≥n.

### Urea y Creatinine
- Relaci√≥n positiva clara: a mayor **creatinina**, mayor **urea**.
- Ambos g√©neros siguen exactamente la misma tendencia.

### EF (fracci√≥n de eyecci√≥n)
- Se nota la concentraci√≥n en el valor **60**.
- No hay una diferencia visible entre g√©neros en este patr√≥n.
- Relaci√≥n inversa tenue con **urea/creatinina**: pacientes con valores altos de estos par√°metros tienden a mostrar **EF** m√°s baja.
""")

st.subheader("¬øCu√°l sexo presenta mayor cantidad de hospitalizaciones?")

# Usa el DF procesado si est√° en sesi√≥n; si no, usa df
df_use = st.session_state.get("df", df)

if "GENDER" not in df_use.columns:
    st.warning("No existe la columna 'GENDER' en el DataFrame.")
else:
    # Normaliza posibles codificaciones de g√©nero
    g = df_use["GENDER"].copy()

    # Si viene como n√∫meros 0/1
    if set(pd.Series(g.dropna().unique())).issubset({0, 1}):
        g = g.map({1: "Masculino", 0: "Femenino"})
    # Si viene como letras M/F
    elif set(pd.Series(g.dropna().astype(str).str.upper().unique())).issubset({"M", "F"}):
        g = g.astype(str).str.upper().map({"M": "Masculino", "F": "Femenino"})
    else:
        # Dejar tal cual, pero convertir a string para evitar problemas
        g = g.astype(str)

    # Conteos
    gender_counts = g.value_counts().rename_axis("G√©nero").reset_index(name="Cantidad")
    st.dataframe(gender_counts, use_container_width=True)

    # Gr√°fico
    sns.set_style("whitegrid")
    fig, ax = plt.subplots(figsize=(6, 4))
    sns.barplot(x="G√©nero", y="Cantidad", data=gender_counts, ax=ax)
    ax.set_title("Distribuci√≥n por sexo", fontsize=14)
    ax.set_xlabel("Sexo", fontsize=12)
    ax.set_ylabel("Cantidad de personas", fontsize=12)

    # Etiquetas encima de las barras
    for i, v in enumerate(gender_counts["Cantidad"]):
        ax.text(i, v, str(int(v)), ha="center", va="bottom", fontsize=10)

    st.pyplot(fig)
    plt.close(fig)

st.subheader("¬øC√≥mo se ve afectada la cantidad de hospitalizaciones por la edad?")

# Usa el DataFrame procesado si est√° en sesi√≥n; si no, usa df
df_use = st.session_state.get("df", df)

if "AGE" not in df_use.columns:
    st.warning("No existe la columna 'AGE' en el DataFrame.")
else:
    # Controles
    bins = st.slider("N√∫mero de bins", 5, 80, 20, 1)

    # Estilo y figura
    sns.set_style("whitegrid")
    fig, ax = plt.subplots(figsize=(10, 6))
    sns.histplot(data=df_use, x="AGE", bins=bins, kde=False, color="blue", ax=ax)

    # Personalizaci√≥n
    ax.set_title("Distribuci√≥n de Edades", fontsize=16)
    ax.set_xlabel("Edad", fontsize=12)
    ax.set_ylabel("Frecuencia", fontsize=12)

    st.pyplot(fig)
    plt.close(fig)
import streamlit as st

st.subheader("Interpretaci√≥n de la distribuci√≥n de hospitalizaciones")

with st.expander("Resumen interpretativo", expanded=True):
    st.markdown("""
La distribuci√≥n de hospitalizaciones presenta un patr√≥n claro y esperado.

**Pico de hospitalizaciones.**  
El rango de edad con mayor n√∫mero de hospitalizaciones se encuentra entre **55 y 63 a√±os**, seguido de cerca por **63 a 68 a√±os**. Esto es coherente con el aumento de la prevalencia de enfermedades cr√≥nicas (hipertensi√≥n, diabetes, cardiovasculares) y la acumulaci√≥n de factores de riesgo a medida que las personas envejecen.

**Asimetr√≠a negativa.**  
Aunque hay hospitalizaciones en todas las edades, la mayor concentraci√≥n ocurre en los grupos de mayor edad. Las personas mayores suelen tener sistemas inmunol√≥gicos m√°s d√©biles y m√∫ltiples comorbilidades, lo que incrementa su vulnerabilidad a infecciones y complicaciones.

**Menor frecuencia en 0‚Äì20 a√±os.**  
Este grupo presenta menos hospitalizaciones porque, en general, ni√±os y adultos j√≥venes tienen un sistema inmune m√°s robusto y menor incidencia de enfermedades cr√≥nicas graves. En ellos, las hospitalizaciones suelen asociarse a accidentes, infecciones agudas o condiciones cong√©nitas.

**Conclusi√≥n.**  
El gr√°fico refleja una **relaci√≥n positiva entre envejecimiento y probabilidad de hospitalizaci√≥n**, explicada por la acumulaci√≥n de riesgos y el deterioro natural del cuerpo.
""")

import streamlit as st
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

st.subheader("¬øExiste relaci√≥n entre la edad y los d√≠as de hospitalizaci√≥n?")

df_use = st.session_state.get("df", df)

xcol = "AGE"
ycol = "DURATION OF STAY"
if xcol not in df_use.columns or ycol not in df_use.columns:
    st.warning(f"Faltan columnas: '{xcol}' o '{ycol}'.")
else:
    # Controles
    alpha = st.slider("Transparencia de puntos (alpha)", 0.05, 1.0, 0.6, 0.05)
    add_trend = st.checkbox("A√±adir l√≠nea de tendencia (regresi√≥n lineal)", value=True)
    corr_method = st.selectbox("Tipo de correlaci√≥n", ["pearson", "spearman"], index=0)

    # Datos sin nulos
    data = df_use[[xcol, ycol]].dropna()

    # Gr√°fico
    sns.set_style("whitegrid")
    fig, ax = plt.subplots(figsize=(8, 6))
    sns.scatterplot(data=data, x=xcol, y=ycol, alpha=alpha, ax=ax)
    if add_trend:
        sns.regplot(data=data, x=xcol, y=ycol, scatter=False, color="red", ax=ax)

    ax.set_title("Relaci√≥n entre Edad y D√≠as de Hospitalizaci√≥n")
    ax.set_xlabel("Edad del paciente")
    ax.set_ylabel("D√≠as de hospitalizaci√≥n")
    ax.grid(True, linestyle="--", alpha=0.5)

    st.pyplot(fig)
    plt.close(fig)

    # Correlaci√≥n
    corr = data[xcol].corr(data[ycol], method=corr_method)
    st.markdown(f"**Correlaci√≥n ({corr_method}):** `{corr:.4f}`")

import streamlit as st
import pandas as pd
from sklearn.model_selection import train_test_split

st.header("3. Dividir conjunto de entrenamiento y prueba")

with st.expander("¬øPor qu√© esta variable objetivo?", expanded=True):
    st.markdown("""
La variable elegida como objetivo es de tipo **num√©rico continuo** y representa el n√∫mero de d√≠as
que un paciente permanece en el hospital. Su predicci√≥n tiene valor cl√≠nico y operativo
(planificaci√≥n de recursos, disponibilidad de camas y asignaci√≥n de personal).
La duraci√≥n est√° influenciada por m√∫ltiples factores del conjunto de datos
(diagn√≥sticos, comorbilidades y resultados de laboratorio).
""")

# DataFrame base
df_use = st.session_state.get("df", df)

# Target y features
target_default = "DURATION OF STAY" if "DURATION OF STAY" in df_use.columns else None
target = st.selectbox("Variable objetivo (y)", options=[c for c in df_use.columns], index=(list(df_use.columns).index(target_default) if target_default else 0))

# Usamos listas de features guardadas o las detectamos
num_features = st.session_state.get("num_features") or df_use.select_dtypes(include="number").columns.tolist()
cat_features = st.session_state.get("cat_features") or [c for c in df_use.columns if c not in num_features]

# Excluir fechas/target si estuvieran
exclude = [c for c in ["D.O.A", "D.O.D", target, "SNO", "MRD No."] if c in df_use.columns]
feat_list = [c for c in (num_features + cat_features) if c in df_use.columns and c not in exclude]

import re
import streamlit as st

df_use = st.session_state.get("df", df).copy()
df_use.columns = df_use.columns.str.strip()  # quita espacios

# --------- lista de columnas a excluir (con variantes) ----------
import re

df_use = st.session_state.get("df", df).copy()
df_use.columns = df_use.columns.str.strip()  # quita espacios

# columnas a eliminar (NO incluir la variable objetivo)
ban_raw = ["D.O.A", "D.O.D", "SNO", "MRD No.", "MRD No"]
def canon(name: str) -> str:
    return re.sub(r"[\W_]+", "", str(name)).lower()

ban_canon = {canon(c) for c in ban_raw}
cols_to_drop = [c for c in df_use.columns if canon(c) in ban_canon]
if cols_to_drop:
    df_use.drop(columns=cols_to_drop, inplace=True)

# >>> Correcci√≥n: excluir el target al recalcular las listas
target = st.session_state.get("target", "DURATION OF STAY")

num_features = df_use.select_dtypes(include="number").columns.tolist()
# quita el target si qued√≥ como num√©rica
num_features = [c for c in num_features if c != target]

# categ√≥ricas = resto sin contar el target
cat_features = [c for c in df_use.columns if c not in num_features + [target]]

# guarda en sesi√≥n
st.session_state["df"] = df_use
st.session_state["num_features"] = num_features
st.session_state["cat_features"] = cat_features

# si despu√©s armas feat_list, filtra otra vez por seguridad:
target = "DURATION OF STAY"
exclude = [c for c in ["D.O.A", "D.O.D", target, "SNO", "MRD No.", "MRD No"] if c in df_use.columns]
feat_list = [c for c in (num_features + cat_features) if c not in exclude]



st.markdown(f"**# de features seleccionadas:** {len(feat_list)}")
st.caption(f"Excluyendo: {', '.join(exclude) if exclude else '‚Äî'}")

# Par√°metros del split
col_a, col_b = st.columns(2)
with col_a:
    test_size = st.slider("Proporci√≥n de test", 0.1, 0.5, 0.3, 0.05)
with col_b:
    random_state = st.number_input("Random state", min_value=0, value=42, step=1)

# Ejecutar split
if target and feat_list:
    X = df_use[feat_list].copy()
    y = df_use[target].copy()

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )

    st.success(f"Split realizado ‚úÖ  |  X_train: {X_train.shape}  |  X_test: {X_test.shape}  |  y_train: {y_train.shape}  |  y_test: {y_test.shape}")

    # Guardar en sesi√≥n para los siguientes pasos
    st.session_state["X_train"] = X_train
    st.session_state["X_test"]  = X_test
    st.session_state["y_train"] = y_train
    st.session_state["y_test"]  = y_test
    st.session_state["target"]  = target
else:
    st.warning("Selecciona la variable objetivo y verifica que existan features disponibles.")


# --- aqu√≠ termina el split y se guardan en session_state ---

import streamlit as st
import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import RobustScaler

import streamlit as st
import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import RobustScaler

st.subheader("üì¶ Preprocesamiento (imputaci√≥n + RobustScaler)")

# 1) Recupera desde el estado (deben haberse creado en el split)
X_train = st.session_state.get("X_train")
X_test  = st.session_state.get("X_test")
num_features_raw = list(st.session_state.get("num_features", []))
cat_features_raw = list(st.session_state.get("cat_features", []))

if X_train is None or X_test is None:
    st.error("Primero realiza el split de entrenamiento/prueba.")
    st.stop()

# 2) Asegura que las listas solo incluyan columnas presentes en X_train
cols_train = set(X_train.columns)
num_features = [c for c in num_features_raw if c in cols_train]
cat_features = [c for c in cat_features_raw if c in cols_train]

# Evita solapamientos
overlap = sorted(set(num_features) & set(cat_features))
if overlap:
    st.warning(f"Columnas en num y cat a la vez (se quitan de cat): {overlap}")
    cat_features = [c for c in cat_features if c not in overlap]

# 3) Construye transformadores
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="mean")),
    ("scaler", RobustScaler())
])
categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent"))
])

transformers = []
if num_features:
    transformers.append(("num", numeric_transformer, num_features))
if cat_features:
    transformers.append(("cat", categorical_transformer, cat_features))

if not transformers:
    st.error("No hay columnas v√°lidas para transformar.")
    st.stop()

# 4) ColumnTransformer y transformaci√≥n
preprocessor = ColumnTransformer(transformers=transformers, remainder="drop")
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed  = preprocessor.transform(X_test)

# 5) Reconstrucci√≥n SEGURA de matrices procesadas a DataFrames
def _to_dense(m):
    try:
        return m.toarray()
    except AttributeError:
        return m

Xtr_vals = _to_dense(X_train_processed)
Xte_vals = _to_dense(X_test_processed)

# Nombres desde el preprocesador reci√©n ajustado
feat_out = None
if hasattr(preprocessor, "get_feature_names_out"):
    try:
        feat_out = list(preprocessor.get_feature_names_out())
        # limpia prefijos 'num__' / 'cat__'
        feat_out = [f.split("__", 1)[1] if "__" in f else f for f in feat_out]
    except Exception:
        feat_out = None

# Si la cantidad de nombres NO coincide con la matriz, usa gen√©ricos
n_cols = Xtr_vals.shape[1]
if feat_out is None or len(feat_out) != n_cols:
    st.warning(
        f"Nombres de features ({0 if feat_out is None else len(feat_out)}) "
        f"‚â† columnas de la matriz ({n_cols}). Se usar√°n nombres gen√©ricos."
    )
    feat_out = [f"feat_{i}" for i in range(n_cols)]

# DataFrames finales
X_train_proc_df = pd.DataFrame(Xtr_vals, columns=feat_out, index=X_train.index)
X_test_proc_df  = pd.DataFrame(Xte_vals,  columns=feat_out, index=X_test.index)

st.success(f"Preprocesamiento OK ¬∑ X_train_proc: {X_train_proc_df.shape} ¬∑ X_test_proc: {X_test_proc_df.shape}")

# 6) Guarda en sesi√≥n para pasos siguientes (PCA/MCA/modelado)
st.session_state["preprocessor"] = preprocessor
st.session_state["X_train_processed"] = X_train_proc_df
st.session_state["X_test_processed"]  = X_test_proc_df



# Recupera objetos necesarios
pre = st.session_state.get("preprocessor")           # ColumnTransformer ya fit
X_train = st.session_state.get("X_train")
X_test  = st.session_state.get("X_test")

# 1) Reconstruir DataFrames desde las matrices procesadas (ndarray -> DataFrame)
def _to_dense(m):
    try:
        return m.toarray()
    except AttributeError:
        return m

Xtr_vals = _to_dense(X_train_processed)
Xte_vals = _to_dense(X_test_processed)

# Nombres desde el preprocesador
try:
    feat_out = list(pre.get_feature_names_out())
    feat_out = [f.split("__", 1)[1] if "__" in f else f for f in feat_out]  # quita 'num__'/'cat__'
except Exception:
    feat_out = [f"feat_{i}" for i in range(Xtr_vals.shape[1])]

X_train_processed = pd.DataFrame(Xtr_vals, columns=feat_out, index=X_train.index)
X_test_processed  = pd.DataFrame(Xte_vals,  columns=feat_out, index=X_test.index)

# Opcional: guardar para siguientes pasos
st.session_state["X_train_processed"] = X_train_processed
st.session_state["X_test_processed"]  = X_test_processed

# 2) Seleccionar las num√©ricas por NOMBRE y hacer PCA
valid_num = [c for c in num_features if c in X_train_processed.columns]

X_train_numericas = X_train_processed[valid_num].copy()
X_test_numericas  = X_test_processed[valid_num].copy()

pca = PCA(n_components=0.70, random_state=42)
Xn_train_pca = pca.fit_transform(X_train_numericas)
Xn_test_pca  = pca.transform(X_test_numericas)

pca_names = [f'PCA{i+1}' for i in range(Xn_train_pca.shape[1])]
Xn_train_pca = pd.DataFrame(Xn_train_pca, columns=pca_names, index=X_train.index)
Xn_test_pca  = pd.DataFrame(Xn_test_pca,  columns=pca_names, index=X_test.index)

st.write(f'PCA: {len(pca_names)} componentes, var. explicada acumulada = {pca.explained_variance_ratio_.sum():.3f}')
